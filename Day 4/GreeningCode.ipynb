{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Greening your Code Practices\n",
        "\n",
        "Here are some practices for writing more energy-efficient code, often referred to as \"greening your code\":\n",
        "\n",
        "1.  **Optimize Data Structures:**\n",
        "    *   When importing your dataset encode the column to the most effective type. E.g. an integer will always be faster than a float so if your numeric column is an integer you shall encode it as integer and not as a float\n",
        "    *   Select data structures appropriate for the task to minimize memory usage and access time.\n",
        "\n",
        "2.  **Reduce Redundancy:**\n",
        "    *   Avoid unnecessary computations. Cache results when possible.\n",
        "    *   Eliminate duplicate code, which can lead to redundant processing.\n",
        "\n",
        "3.  **Minimize I/O Operations:**\n",
        "    *   Reading and writing data to disk or across a network consumes significant energy.\n",
        "    *   Buffer data when possible to perform fewer, larger I/O operations instead of many small ones.\n",
        "    *   Only read the data you actually need.\n",
        "\n",
        "4.  **Use Efficient Libraries and Frameworks:**\n",
        "    *   Leverage well-optimised libraries and frameworks that are designed for performance and efficiency.\n",
        "    *   For computationally intensive tasks, consider libraries that utilise underlying hardware efficiently (e.g., NumPy, TensorFlow, PyTorch).\n",
        "\n",
        "5.  **Manage Memory Effectively:**\n",
        "    *   Release memory when it's no longer needed to reduce the memory footprint and the overhead of garbage collection.\n",
        "    *   If you are done with the analysis of an object just remove it\n",
        "    *   Avoid creating large, unnecessary objects.\n",
        "\n",
        "6.  **Parallelise and Distribute When Appropriate:**\n",
        "    *   For suitable tasks, parallelising or distributing computations across multiple cores or machines can potentially reduce the overall execution time and, in some cases, energy consumption (though this can be complex and depends on the task and infrastructure).\n",
        "\n",
        "7.  **Profile and Measure:**\n",
        "    *   Use profiling tools to identify bottlenecks in your code where most of the execution time and resource consumption occur. Focus your optimization efforts on these areas.\n",
        "\n",
        "8. **Prototyping:**\n",
        "    *   When working with large dataset, don't build up your code on the totality of the dataset but create a very small subset on which to test your code and then progressivly test it across larger and larger sections.\n",
        "\n",
        "9. **Be Mindful of Loops:**\n",
        "    *   Optimise loops to reduce the number of iterations and the work performed within each iteration.\n",
        "\n",
        "10. **Code Review:**\n",
        "    *   Review code with efficiency in mind, identifying potential areas for optimization.\n",
        "\n",
        "\n",
        "   \n",
        "\\\\\n",
        "These are just some examples and not always is possible to implement them becasue a lot will depend on the hardware you are working on and the tasks you are trying to perform. The general suggestion is to write your first working draft and then refine it to be more efficient/consume less Co2.\n",
        "\n",
        "\\\\\n",
        "\n",
        "In the code below we are going to see three quick examples on how text analysis related code can be optimised.\n",
        "\n",
        "All code is using the EmissionTracker from the Codecarbon package to check how changing the code impact the carbon emissions.\n",
        "\n",
        "\\\\\n",
        "\n",
        "One caveat is that these data will always be an estimate and will be more accurate if you are going to run things locally.\n",
        "\n",
        "After the examples there are three challenges for you to pick from to test yourself.\n"
      ],
      "metadata": {
        "id": "watXDvrUSKdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the extra package we need  \n"
      ],
      "metadata": {
        "id": "JbOR5ldxUq5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oAnOcc4iV-6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1: Data Upload\n",
        "Below is some code that will import a farily large dataset.\n",
        "For our research though we only need some of the columns so can we improve the data upload part by directly importing only what you need.\n",
        "\n",
        "**NB** if you are working with multiple files it is a good idea to clone the whole repo with !git clone rather than access the different files separately\n"
      ],
      "metadata": {
        "id": "4EUguFWRSSFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import EmissionsTracker # this\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/kingsdigitallab/dh-rse-summer-school-2025/refs/heads/main/Day%204/Data/ParishTokenised.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Parish = pd.read_csv(url)\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for importing the whole dataset: {emissions} kg\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I0CB6GZMS9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is ok but can we see a difference when we import only the variable I am interested in?"
      ],
      "metadata": {
        "id": "JhbQ_4Y5SJyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/kingsdigitallab/dh-rse-summer-school-2025/refs/heads/main/Day%204/Data/ParishTokenised.csv\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Parish = pd.read_csv(url, usecols=[\"title\", \"Type\", \"Area\", \"Parish\"])\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for optimized data import: {emissions} kg\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xdf7wEh5XgB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why it matters:\n",
        "\n",
        "- Reduces memory use and loading time.\n",
        "\n",
        "- Less data = less processing = lower energy usage.\n",
        "\n"
      ],
      "metadata": {
        "id": "kD0wfpDFar1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's follow the advices we preached so since we have finished with this Parish object we remove it."
      ],
      "metadata": {
        "id": "7baEYVGkdeVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del Parish"
      ],
      "metadata": {
        "id": "bwo9JKkLdc4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example2: Data Wrangling _ Avoid Uneccessary Loops\n",
        "\n",
        "Use case: You want to clean up a column by removing punctuation and formatted spaces (e.g. going to the next line) from text.\n",
        "\n",
        "First we import the dataset we want to use for this"
      ],
      "metadata": {
        "id": "K1cFepiTbDV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/kingsdigitallab/dh-rse-summer-school-2025/refs/heads/main/Day%204/Data/Parish.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Parish2 = pd.read_csv(url)\n",
        "\n",
        "Parish2['text'] = Parish2['text'].fillna('').astype(str) #Remove empty texts"
      ],
      "metadata": {
        "id": "upK2vjkwbhbC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "cleaned = []\n",
        "for item in Parish2[\"text\"]:\n",
        "    cleaned.append(\"\".join([c for c in item if c.isalnum() or c.isspace()]))# list comprehension to loop through each caracter and keep only space or alphanumeric removing all punctuation and space-formatting\n",
        "Parish2[\"text_cleaned\"] = cleaned\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for looping through words to remove numbers and spaces: {emissions} kg\")"
      ],
      "metadata": {
        "id": "5xKsXD_sakWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Parish2[\"text_cleaned\"].iloc[1:3])"
      ],
      "metadata": {
        "id": "tJbqeFIIeVmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happen when I used functions from re instead"
      ],
      "metadata": {
        "id": "q1oRSAyCSWbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "import re\n",
        "\n",
        "Parish2[\"text_cleaned\"] = Parish2[\"text\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for optimized cleaning function using regex: {emissions} kg\")"
      ],
      "metadata": {
        "id": "zC3hLRnmcgf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Parish2[\"text_cleaned\"].iloc[1:3])"
      ],
      "metadata": {
        "id": "p2XVZPhCgNSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why it matters:\n",
        "\n",
        "- Uses optimized internal functions instead of manual Python loops.\n",
        "\n",
        "- Faster and uses less CPU power."
      ],
      "metadata": {
        "id": "8V1jodAjLOP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example3: Text Analysis\n",
        "\n",
        "In our third example we are going to see examples on how to improve the tokenisation aspect using the cache (this would work if you are using a corpus where senences words would repeat themselves).\n"
      ],
      "metadata": {
        "id": "KaOE1uBwL59H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "id": "79lbIvCpMHbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "Parish2[\"tokens\"] = Parish2[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for tokenisation {emissions} kg\")"
      ],
      "metadata": {
        "id": "eHRu7tYMMwL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "from functools import lru_cache\n",
        "# Use caching to avoid re-tokenizing identical texts\n",
        "@lru_cache(maxsize=None)\n",
        "def tokenize_cached(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "# Apply the cached function using .map for better performance\n",
        "Parish2[\"tokens\"] = Parish2[\"text\"].map(tokenize_cached)\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for cached tokenisation: {emissions} kg\")"
      ],
      "metadata": {
        "id": "or1uWUZHNKEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok not very much difference in our example, this should work better for things like social media data that tend to repeat themselves more often.\n",
        "\n",
        "Before moving to the challenge let's keep following our own advices and remove the object Parish2 that we do not need anymore"
      ],
      "metadata": {
        "id": "hoiGt0FFi-_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del Parish2"
      ],
      "metadata": {
        "id": "sIWhdP5UODf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why it matters:\n",
        "- Avoids repeating the same work â€“ great when there are duplicate or very similar records.\n",
        "- Lowers CPU usage, which is not only faster but more energy-efficient"
      ],
      "metadata": {
        "id": "r9zVrsgUPFH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge\n",
        "\n",
        "Below you can find some code that will import a dataset of old Trump tweets. Right now the esitmated Co2 is **5.5201969058160406e-05 kg** lets see if follwing the advice we just went through we can make it better.\n",
        "\n",
        "Let's say that we are only interested in the date created_at and text not the id"
      ],
      "metadata": {
        "id": "2WyJoisATIti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages needed Run this just in case the kernel restarted otherwise they sould be already mounted\n",
        "import pandas as pd\n",
        "import re\n",
        "#!pip install codecarbon\n",
        "from codecarbon import EmissionsTracker\n",
        "from functools import lru_cache\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "DjPfalqtkmc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this again check the cost so we wrap it around the whole code\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "#Define the URl\n",
        "url = \"https://raw.githubusercontent.com/kingsdigitallab/dh-rse-summer-school-2025/refs/heads/main/Day%204/Data/trump-tweet-archive.csv\"\n",
        "\n",
        "# Create the object\n",
        "Tweets = pd.read_csv(url)\n",
        "Tweets['text'] = Tweets['text'].fillna('').astype(str) #Remove empty texts\n",
        "\n",
        "# Remove punctuation\n",
        "cleaned = []\n",
        "for item in Tweets[\"text\"]:\n",
        "    cleaned.append(\"\".join([c for c in item if c.isalnum() or c.isspace()]))# list comprehension to loop through each caracter and keep only space or alphanumeric removing all punctuation and space-formatting\n",
        "Tweets[\"text_cleaned\"] = cleaned\n",
        "\n",
        "#Tokenise\n",
        "Tweets[\"tokens\"] = Tweets[\"text\"].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for working with Trump Tweets: {emissions} kg\")"
      ],
      "metadata": {
        "id": "_OpvtjlOTB5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution\n"
      ],
      "metadata": {
        "id": "KIT3RgCcMsrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# this again check the cost so we wrap it around the whole code\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "#Define the URl\n",
        "url = \"https://raw.githubusercontent.com/kingsdigitallab/dh-rse-summer-school-2025/refs/heads/main/Day%204/Data/trump-tweet-archive.csv\"\n",
        "# Create the object\n",
        "Tweets = pd.read_csv(url, usecols=[\"created_at\", \"text\"])\n",
        "Tweets['text'] = Tweets['text'].fillna('').astype(str) #Remove empty texts\n",
        "\n",
        "\n",
        "# Remove punctuation\n",
        "Tweets[\"text_cleaned\"] = Tweets[\"text\"].str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "\n",
        "\n",
        "#Tokenise\n",
        "# Use caching to avoid re-tokenizing identical texts\n",
        "@lru_cache(maxsize=None)\n",
        "def tokenize_cached(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "# Apply the cached function using .map for better performance\n",
        "Tweets[\"tokens\"] = Tweets[\"text\"].map(tokenize_cached)\n",
        "\n",
        "\n",
        "\n",
        "emissions = tracker.stop()\n",
        "print(f\"Estimated CO2 emissions for working with Trump Tweets: {emissions} kg\")"
      ],
      "metadata": {
        "id": "EqWGOSP-mrLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NB** Remember the advice about prototyping so do not try to edit it straight from the cell above but do bit by bit"
      ],
      "metadata": {
        "id": "mWUzfE9jkUYT"
      }
    }
  ]
}